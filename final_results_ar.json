{
  "model": "ar",
  "best_config": {
    "model_name": "aubmindlab/bert-base-arabertv02",
    "train_lang": "ar",
    "learning_rate": 1e-05,
    "seed": 42,
    "val_accuracy": 0.6785714285714286,
    "test_accuracy": 0.5679611650485437
  },
  "all_results": [
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "en",
      "val_accuracy": 0.5335689045936396,
      "val_f1": 0.5147209512346275,
      "val_precision": 0.5196042146907665,
      "val_recall": 0.5335689045936396,
      "test_accuracy": 0.5202702702702703,
      "test_f1": 0.5071723242909716,
      "test_precision": 0.5209113552960302,
      "test_recall": 0.5202702702702703,
      "learning_rate": 1e-05,
      "seed": 42,
      "checkpoint_dir": "./results_bert-base-arabertv02_en_seed42"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "en",
      "val_accuracy": 0.5547703180212014,
      "val_f1": 0.5335051200928111,
      "val_precision": 0.5728575394171915,
      "val_recall": 0.5547703180212014,
      "test_accuracy": 0.5304054054054054,
      "test_f1": 0.5214871464871464,
      "test_precision": 0.5269151995409768,
      "test_recall": 0.5304054054054054,
      "learning_rate": 1e-05,
      "seed": 43,
      "checkpoint_dir": "./results_bert-base-arabertv02_en_seed43"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "en",
      "val_accuracy": 0.5300353356890459,
      "val_f1": 0.5162094997535521,
      "val_precision": 0.509580972841208,
      "val_recall": 0.5300353356890459,
      "test_accuracy": 0.5304054054054054,
      "test_f1": 0.5219586131225171,
      "test_precision": 0.5233100558663029,
      "test_recall": 0.5304054054054054,
      "learning_rate": 1e-05,
      "seed": 44,
      "checkpoint_dir": "./results_bert-base-arabertv02_en_seed44"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "en",
      "val_accuracy": 0.5547703180212014,
      "val_f1": 0.5404933585968074,
      "val_precision": 0.5332539533039791,
      "val_recall": 0.5547703180212014,
      "test_accuracy": 0.527027027027027,
      "test_f1": 0.5183062526812526,
      "test_precision": 0.520942894140049,
      "test_recall": 0.527027027027027,
      "learning_rate": 1e-05,
      "seed": 45,
      "checkpoint_dir": "./results_bert-base-arabertv02_en_seed45"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "en",
      "val_accuracy": 0.5477031802120141,
      "val_f1": 0.5291692332838095,
      "val_precision": 0.5305769779511308,
      "val_recall": 0.5477031802120141,
      "test_accuracy": 0.5472972972972973,
      "test_f1": 0.5383943533951234,
      "test_precision": 0.5408117882517259,
      "test_recall": 0.5472972972972973,
      "learning_rate": 1e-05,
      "seed": 46,
      "checkpoint_dir": "./results_bert-base-arabertv02_en_seed46"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "en",
      "val_accuracy": 0.5371024734982333,
      "val_f1": 0.4931425512170704,
      "val_precision": 0.5287151127560666,
      "val_recall": 0.5371024734982333,
      "test_accuracy": 0.5202702702702703,
      "test_f1": 0.4977410485706947,
      "test_precision": 0.5285343627530282,
      "test_recall": 0.5202702702702703,
      "learning_rate": 1e-06,
      "seed": 42,
      "checkpoint_dir": "./results_bert-base-arabertv02_en_seed42"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "en",
      "val_accuracy": 0.5335689045936396,
      "val_f1": 0.4682231138619458,
      "val_precision": 0.46713542164072197,
      "val_recall": 0.5335689045936396,
      "test_accuracy": 0.4831081081081081,
      "test_f1": 0.4349483771080867,
      "test_precision": 0.4796693540743959,
      "test_recall": 0.4831081081081081,
      "learning_rate": 1e-06,
      "seed": 43,
      "checkpoint_dir": "./results_bert-base-arabertv02_en_seed43"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "en",
      "val_accuracy": 0.5123674911660777,
      "val_f1": 0.4746494904263184,
      "val_precision": 0.49370624708313887,
      "val_recall": 0.5123674911660777,
      "test_accuracy": 0.4864864864864865,
      "test_f1": 0.4671587320230199,
      "test_precision": 0.49061635315591,
      "test_recall": 0.4864864864864865,
      "learning_rate": 1e-06,
      "seed": 44,
      "checkpoint_dir": "./results_bert-base-arabertv02_en_seed44"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "en",
      "val_accuracy": 0.5123674911660777,
      "val_f1": 0.47033237139303885,
      "val_precision": 0.45743519455605003,
      "val_recall": 0.5123674911660777,
      "test_accuracy": 0.5135135135135135,
      "test_f1": 0.4774886750235233,
      "test_precision": 0.4618016118544939,
      "test_recall": 0.5135135135135135,
      "learning_rate": 1e-06,
      "seed": 45,
      "checkpoint_dir": "./results_bert-base-arabertv02_en_seed45"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "en",
      "val_accuracy": 0.5371024734982333,
      "val_f1": 0.472799686192382,
      "val_precision": 0.45571345525820006,
      "val_recall": 0.5371024734982333,
      "test_accuracy": 0.4966216216216216,
      "test_f1": 0.45685983295647686,
      "test_precision": 0.5473175768159195,
      "test_recall": 0.4966216216216216,
      "learning_rate": 1e-06,
      "seed": 46,
      "checkpoint_dir": "./results_bert-base-arabertv02_en_seed46"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "en",
      "val_accuracy": 0.29328621908127206,
      "val_f1": 0.2310242216248835,
      "val_precision": 0.33219458091304005,
      "val_recall": 0.29328621908127206,
      "test_accuracy": 0.32432432432432434,
      "test_f1": 0.2732639906221369,
      "test_precision": 0.323762861079783,
      "test_recall": 0.32432432432432434,
      "learning_rate": 1e-07,
      "seed": 42,
      "checkpoint_dir": "./results_bert-base-arabertv02_en_seed42"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "en",
      "val_accuracy": 0.31448763250883394,
      "val_f1": 0.22952524682918654,
      "val_precision": 0.19434014304492142,
      "val_recall": 0.31448763250883394,
      "test_accuracy": 0.30743243243243246,
      "test_f1": 0.2362290131040131,
      "test_precision": 0.42265298084970215,
      "test_recall": 0.30743243243243246,
      "learning_rate": 1e-07,
      "seed": 43,
      "checkpoint_dir": "./results_bert-base-arabertv02_en_seed43"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "en",
      "val_accuracy": 0.3710247349823322,
      "val_f1": 0.3115378266026611,
      "val_precision": 0.32991544674406864,
      "val_recall": 0.3710247349823322,
      "test_accuracy": 0.32432432432432434,
      "test_f1": 0.2782122876300958,
      "test_precision": 0.30902270935680054,
      "test_recall": 0.32432432432432434,
      "learning_rate": 1e-07,
      "seed": 44,
      "checkpoint_dir": "./results_bert-base-arabertv02_en_seed44"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "en",
      "val_accuracy": 0.3745583038869258,
      "val_f1": 0.3391291745069327,
      "val_precision": 0.3788923852019981,
      "val_recall": 0.3745583038869258,
      "test_accuracy": 0.3885135135135135,
      "test_f1": 0.32110844786901127,
      "test_precision": 0.29631733892297274,
      "test_recall": 0.3885135135135135,
      "learning_rate": 1e-07,
      "seed": 45,
      "checkpoint_dir": "./results_bert-base-arabertv02_en_seed45"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "en",
      "val_accuracy": 0.3568904593639576,
      "val_f1": 0.28858627838424816,
      "val_precision": 0.44284094218721437,
      "val_recall": 0.3568904593639576,
      "test_accuracy": 0.3547297297297297,
      "test_f1": 0.2852641596837378,
      "test_precision": 0.4654584018411169,
      "test_recall": 0.3547297297297297,
      "learning_rate": 1e-07,
      "seed": 46,
      "checkpoint_dir": "./results_bert-base-arabertv02_en_seed46"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ar",
      "val_accuracy": 0.6785714285714286,
      "val_f1": 0.6813675967899597,
      "val_precision": 0.695495300531413,
      "val_recall": 0.6785714285714286,
      "test_accuracy": 0.5679611650485437,
      "test_f1": 0.5664109788317786,
      "test_precision": 0.5764840514346234,
      "test_recall": 0.5679611650485437,
      "learning_rate": 1e-05,
      "seed": 42,
      "checkpoint_dir": "./results_bert-base-arabertv02_ar_seed42"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ar",
      "val_accuracy": 0.6479591836734694,
      "val_f1": 0.6457818223647915,
      "val_precision": 0.6553440618822235,
      "val_recall": 0.6479591836734694,
      "test_accuracy": 0.587378640776699,
      "test_f1": 0.5818182788260226,
      "test_precision": 0.6017856304103786,
      "test_recall": 0.587378640776699,
      "learning_rate": 1e-05,
      "seed": 43,
      "checkpoint_dir": "./results_bert-base-arabertv02_ar_seed43"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ar",
      "val_accuracy": 0.6683673469387755,
      "val_f1": 0.6702386266342557,
      "val_precision": 0.6757774538386784,
      "val_recall": 0.6683673469387755,
      "test_accuracy": 0.5970873786407767,
      "test_f1": 0.59773197581469,
      "test_precision": 0.6082411266013895,
      "test_recall": 0.5970873786407767,
      "learning_rate": 1e-05,
      "seed": 44,
      "checkpoint_dir": "./results_bert-base-arabertv02_ar_seed44"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ar",
      "val_accuracy": 0.6479591836734694,
      "val_f1": 0.6487275046677636,
      "val_precision": 0.6640124383086567,
      "val_recall": 0.6479591836734694,
      "test_accuracy": 0.587378640776699,
      "test_f1": 0.5859739649181397,
      "test_precision": 0.6054595889547346,
      "test_recall": 0.587378640776699,
      "learning_rate": 1e-05,
      "seed": 45,
      "checkpoint_dir": "./results_bert-base-arabertv02_ar_seed45"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ar",
      "val_accuracy": 0.6530612244897959,
      "val_f1": 0.6530877136584342,
      "val_precision": 0.6604551344347263,
      "val_recall": 0.6530612244897959,
      "test_accuracy": 0.6019417475728155,
      "test_f1": 0.6023314194476952,
      "test_precision": 0.616412920366264,
      "test_recall": 0.6019417475728155,
      "learning_rate": 1e-05,
      "seed": 46,
      "checkpoint_dir": "./results_bert-base-arabertv02_ar_seed46"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ar",
      "val_accuracy": 0.5153061224489796,
      "val_f1": 0.5036828719583071,
      "val_precision": 0.6131801995913495,
      "val_recall": 0.5153061224489796,
      "test_accuracy": 0.441747572815534,
      "test_f1": 0.42933052866624655,
      "test_precision": 0.47701584750563153,
      "test_recall": 0.441747572815534,
      "learning_rate": 1e-06,
      "seed": 42,
      "checkpoint_dir": "./results_bert-base-arabertv02_ar_seed42"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ar",
      "val_accuracy": 0.4846938775510204,
      "val_f1": 0.44913948508933865,
      "val_precision": 0.4332046181533603,
      "val_recall": 0.4846938775510204,
      "test_accuracy": 0.46116504854368934,
      "test_f1": 0.43072545709463317,
      "test_precision": 0.41369672621765025,
      "test_recall": 0.46116504854368934,
      "learning_rate": 1e-06,
      "seed": 43,
      "checkpoint_dir": "./results_bert-base-arabertv02_ar_seed43"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ar",
      "val_accuracy": 0.4846938775510204,
      "val_f1": 0.4593540872570226,
      "val_precision": 0.4750560954074715,
      "val_recall": 0.4846938775510204,
      "test_accuracy": 0.45145631067961167,
      "test_f1": 0.4361813794508873,
      "test_precision": 0.47819508812286465,
      "test_recall": 0.45145631067961167,
      "learning_rate": 1e-06,
      "seed": 44,
      "checkpoint_dir": "./results_bert-base-arabertv02_ar_seed44"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ar",
      "val_accuracy": 0.5153061224489796,
      "val_f1": 0.49518324928315005,
      "val_precision": 0.5433078592850905,
      "val_recall": 0.5153061224489796,
      "test_accuracy": 0.470873786407767,
      "test_f1": 0.4620043727809102,
      "test_precision": 0.4873666093359446,
      "test_recall": 0.470873786407767,
      "learning_rate": 1e-06,
      "seed": 45,
      "checkpoint_dir": "./results_bert-base-arabertv02_ar_seed45"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ar",
      "val_accuracy": 0.5,
      "val_f1": 0.481468678329919,
      "val_precision": 0.5150869353952762,
      "val_recall": 0.5,
      "test_accuracy": 0.529126213592233,
      "test_f1": 0.5133075414509292,
      "test_precision": 0.5433602951256901,
      "test_recall": 0.529126213592233,
      "learning_rate": 1e-06,
      "seed": 46,
      "checkpoint_dir": "./results_bert-base-arabertv02_ar_seed46"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ar",
      "val_accuracy": 0.19387755102040816,
      "val_f1": 0.11486552915213959,
      "val_precision": 0.18803081115165787,
      "val_recall": 0.19387755102040816,
      "test_accuracy": 0.22815533980582525,
      "test_f1": 0.1525271740495196,
      "test_precision": 0.22278355593956076,
      "test_recall": 0.22815533980582525,
      "learning_rate": 1e-07,
      "seed": 42,
      "checkpoint_dir": "./results_bert-base-arabertv02_ar_seed42"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ar",
      "val_accuracy": 0.30612244897959184,
      "val_f1": 0.2650971195931789,
      "val_precision": 0.3399864997935919,
      "val_recall": 0.30612244897959184,
      "test_accuracy": 0.30097087378640774,
      "test_f1": 0.25403805458917544,
      "test_precision": 0.36444789587354737,
      "test_recall": 0.30097087378640774,
      "learning_rate": 1e-07,
      "seed": 43,
      "checkpoint_dir": "./results_bert-base-arabertv02_ar_seed43"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ar",
      "val_accuracy": 0.25510204081632654,
      "val_f1": 0.19829206976545277,
      "val_precision": 0.25101293264558566,
      "val_recall": 0.25510204081632654,
      "test_accuracy": 0.2815533980582524,
      "test_f1": 0.21680905223491748,
      "test_precision": 0.3139702951199126,
      "test_recall": 0.2815533980582524,
      "learning_rate": 1e-07,
      "seed": 44,
      "checkpoint_dir": "./results_bert-base-arabertv02_ar_seed44"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ar",
      "val_accuracy": 0.29591836734693877,
      "val_f1": 0.2785428590695418,
      "val_precision": 0.3349702586649937,
      "val_recall": 0.29591836734693877,
      "test_accuracy": 0.2669902912621359,
      "test_f1": 0.24997952409367843,
      "test_precision": 0.3160471470648907,
      "test_recall": 0.2669902912621359,
      "learning_rate": 1e-07,
      "seed": 45,
      "checkpoint_dir": "./results_bert-base-arabertv02_ar_seed45"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ar",
      "val_accuracy": 0.2857142857142857,
      "val_f1": 0.25770975056689344,
      "val_precision": 0.30588977349181434,
      "val_recall": 0.2857142857142857,
      "test_accuracy": 0.2815533980582524,
      "test_f1": 0.24119208415797985,
      "test_precision": 0.25920684981685865,
      "test_recall": 0.2815533980582524,
      "learning_rate": 1e-07,
      "seed": 46,
      "checkpoint_dir": "./results_bert-base-arabertv02_ar_seed46"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "hi",
      "val_accuracy": 0.3624161073825503,
      "val_f1": 0.2996232541605318,
      "val_precision": 0.31183666269182614,
      "val_recall": 0.3624161073825503,
      "test_accuracy": 0.3496932515337423,
      "test_f1": 0.2776542856302943,
      "test_precision": 0.4156831136496053,
      "test_recall": 0.3496932515337423,
      "learning_rate": 1e-05,
      "seed": 42,
      "checkpoint_dir": "./results_bert-base-arabertv02_hi_seed42"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "hi",
      "val_accuracy": 0.3691275167785235,
      "val_f1": 0.32651415616968815,
      "val_precision": 0.40330924109447597,
      "val_recall": 0.3691275167785235,
      "test_accuracy": 0.37423312883435583,
      "test_f1": 0.35188176394898185,
      "test_precision": 0.39111761142936563,
      "test_recall": 0.37423312883435583,
      "learning_rate": 1e-05,
      "seed": 43,
      "checkpoint_dir": "./results_bert-base-arabertv02_hi_seed43"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "hi",
      "val_accuracy": 0.3691275167785235,
      "val_f1": 0.2975190912540613,
      "val_precision": 0.25065624978355877,
      "val_recall": 0.3691275167785235,
      "test_accuracy": 0.37423312883435583,
      "test_f1": 0.29433490357179803,
      "test_precision": 0.2504576626582587,
      "test_recall": 0.37423312883435583,
      "learning_rate": 1e-05,
      "seed": 44,
      "checkpoint_dir": "./results_bert-base-arabertv02_hi_seed44"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "hi",
      "val_accuracy": 0.35570469798657717,
      "val_f1": 0.26902363339254276,
      "val_precision": 0.24460453361872656,
      "val_recall": 0.35570469798657717,
      "test_accuracy": 0.3619631901840491,
      "test_f1": 0.27536227988219103,
      "test_precision": 0.25973729746735885,
      "test_recall": 0.3619631901840491,
      "learning_rate": 1e-05,
      "seed": 45,
      "checkpoint_dir": "./results_bert-base-arabertv02_hi_seed45"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "hi",
      "val_accuracy": 0.3825503355704698,
      "val_f1": 0.3457668235548169,
      "val_precision": 0.5206878044191585,
      "val_recall": 0.3825503355704698,
      "test_accuracy": 0.3496932515337423,
      "test_f1": 0.3242138864372205,
      "test_precision": 0.3746210697672264,
      "test_recall": 0.3496932515337423,
      "learning_rate": 1e-05,
      "seed": 46,
      "checkpoint_dir": "./results_bert-base-arabertv02_hi_seed46"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "hi",
      "val_accuracy": 0.348993288590604,
      "val_f1": 0.33557922008278285,
      "val_precision": 0.3612070343064326,
      "val_recall": 0.348993288590604,
      "test_accuracy": 0.36809815950920244,
      "test_f1": 0.33580977845866394,
      "test_precision": 0.40054614449033615,
      "test_recall": 0.36809815950920244,
      "learning_rate": 1e-06,
      "seed": 42,
      "checkpoint_dir": "./results_bert-base-arabertv02_hi_seed42"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "hi",
      "val_accuracy": 0.3422818791946309,
      "val_f1": 0.3114731038493028,
      "val_precision": 0.37185897001333246,
      "val_recall": 0.3422818791946309,
      "test_accuracy": 0.34355828220858897,
      "test_f1": 0.3035475301036368,
      "test_precision": 0.29860346993700204,
      "test_recall": 0.34355828220858897,
      "learning_rate": 1e-06,
      "seed": 43,
      "checkpoint_dir": "./results_bert-base-arabertv02_hi_seed43"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "hi",
      "val_accuracy": 0.3825503355704698,
      "val_f1": 0.3424540498044855,
      "val_precision": 0.3804514367602237,
      "val_recall": 0.3825503355704698,
      "test_accuracy": 0.3803680981595092,
      "test_f1": 0.3364974186233065,
      "test_precision": 0.3302743201132771,
      "test_recall": 0.3803680981595092,
      "learning_rate": 1e-06,
      "seed": 44,
      "checkpoint_dir": "./results_bert-base-arabertv02_hi_seed44"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "hi",
      "val_accuracy": 0.3221476510067114,
      "val_f1": 0.2863191334916679,
      "val_precision": 0.2863364311804896,
      "val_recall": 0.3221476510067114,
      "test_accuracy": 0.37423312883435583,
      "test_f1": 0.3472178782896935,
      "test_precision": 0.3778049193626681,
      "test_recall": 0.37423312883435583,
      "learning_rate": 1e-06,
      "seed": 45,
      "checkpoint_dir": "./results_bert-base-arabertv02_hi_seed45"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "hi",
      "val_accuracy": 0.35570469798657717,
      "val_f1": 0.30726369981172114,
      "val_precision": 0.36047369326775897,
      "val_recall": 0.35570469798657717,
      "test_accuracy": 0.3128834355828221,
      "test_f1": 0.27137812646613,
      "test_precision": 0.5551970270086671,
      "test_recall": 0.3128834355828221,
      "learning_rate": 1e-06,
      "seed": 46,
      "checkpoint_dir": "./results_bert-base-arabertv02_hi_seed46"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "hi",
      "val_accuracy": 0.2751677852348993,
      "val_f1": 0.2098287384536799,
      "val_precision": 0.1737690422925322,
      "val_recall": 0.2751677852348993,
      "test_accuracy": 0.27607361963190186,
      "test_f1": 0.21549380488391676,
      "test_precision": 0.18255034409774237,
      "test_recall": 0.27607361963190186,
      "learning_rate": 1e-07,
      "seed": 42,
      "checkpoint_dir": "./results_bert-base-arabertv02_hi_seed42"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "hi",
      "val_accuracy": 0.3087248322147651,
      "val_f1": 0.19621409668004805,
      "val_precision": 0.15918447916079184,
      "val_recall": 0.3087248322147651,
      "test_accuracy": 0.2331288343558282,
      "test_f1": 0.14868886617144886,
      "test_precision": 0.13564788994236845,
      "test_recall": 0.2331288343558282,
      "learning_rate": 1e-07,
      "seed": 43,
      "checkpoint_dir": "./results_bert-base-arabertv02_hi_seed43"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "hi",
      "val_accuracy": 0.26174496644295303,
      "val_f1": 0.11742048438867815,
      "val_precision": 0.12678128160338328,
      "val_recall": 0.26174496644295303,
      "test_accuracy": 0.18404907975460122,
      "test_f1": 0.09558539541919912,
      "test_precision": 0.34564259871008335,
      "test_recall": 0.18404907975460122,
      "learning_rate": 1e-07,
      "seed": 44,
      "checkpoint_dir": "./results_bert-base-arabertv02_hi_seed44"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "hi",
      "val_accuracy": 0.22818791946308725,
      "val_f1": 0.17601424365571347,
      "val_precision": 0.22251319994790392,
      "val_recall": 0.22818791946308725,
      "test_accuracy": 0.24539877300613497,
      "test_f1": 0.2013162976071497,
      "test_precision": 0.224943859033335,
      "test_recall": 0.24539877300613497,
      "learning_rate": 1e-07,
      "seed": 45,
      "checkpoint_dir": "./results_bert-base-arabertv02_hi_seed45"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "hi",
      "val_accuracy": 0.2483221476510067,
      "val_f1": 0.18529015260850684,
      "val_precision": 0.20520933205496963,
      "val_recall": 0.2483221476510067,
      "test_accuracy": 0.2085889570552147,
      "test_f1": 0.1516509034478542,
      "test_precision": 0.22379985953605588,
      "test_recall": 0.2085889570552147,
      "learning_rate": 1e-07,
      "seed": 46,
      "checkpoint_dir": "./results_bert-base-arabertv02_hi_seed46"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "fr",
      "val_accuracy": 0.45454545454545453,
      "val_f1": 0.44958383671293545,
      "val_precision": 0.5077673706764616,
      "val_recall": 0.45454545454545453,
      "test_accuracy": 0.4918918918918919,
      "test_f1": 0.48238777330698557,
      "test_precision": 0.4974097741171338,
      "test_recall": 0.4918918918918919,
      "learning_rate": 1e-05,
      "seed": 42,
      "checkpoint_dir": "./results_bert-base-arabertv02_fr_seed42"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "fr",
      "val_accuracy": 0.5151515151515151,
      "val_f1": 0.4867497429357894,
      "val_precision": 0.5160546169947584,
      "val_recall": 0.5151515151515151,
      "test_accuracy": 0.5027027027027027,
      "test_f1": 0.4764093700580187,
      "test_precision": 0.4856952541235314,
      "test_recall": 0.5027027027027027,
      "learning_rate": 1e-05,
      "seed": 43,
      "checkpoint_dir": "./results_bert-base-arabertv02_fr_seed43"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "fr",
      "val_accuracy": 0.4484848484848485,
      "val_f1": 0.3976792601338056,
      "val_precision": 0.3832018885727326,
      "val_recall": 0.4484848484848485,
      "test_accuracy": 0.43783783783783786,
      "test_f1": 0.4007630379970806,
      "test_precision": 0.37540131040131036,
      "test_recall": 0.43783783783783786,
      "learning_rate": 1e-05,
      "seed": 44,
      "checkpoint_dir": "./results_bert-base-arabertv02_fr_seed44"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "fr",
      "val_accuracy": 0.47878787878787876,
      "val_f1": 0.4570543980889246,
      "val_precision": 0.47175211316198035,
      "val_recall": 0.47878787878787876,
      "test_accuracy": 0.5243243243243243,
      "test_f1": 0.5122251029309852,
      "test_precision": 0.5568074708615249,
      "test_recall": 0.5243243243243243,
      "learning_rate": 1e-05,
      "seed": 45,
      "checkpoint_dir": "./results_bert-base-arabertv02_fr_seed45"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "fr",
      "val_accuracy": 0.4666666666666667,
      "val_f1": 0.42501534322366036,
      "val_precision": 0.45725282847234067,
      "val_recall": 0.4666666666666667,
      "test_accuracy": 0.4918918918918919,
      "test_f1": 0.45728100825862755,
      "test_precision": 0.5166397432969064,
      "test_recall": 0.4918918918918919,
      "learning_rate": 1e-05,
      "seed": 46,
      "checkpoint_dir": "./results_bert-base-arabertv02_fr_seed46"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "fr",
      "val_accuracy": 0.37575757575757573,
      "val_f1": 0.3072467180125338,
      "val_precision": 0.2883555325913162,
      "val_recall": 0.37575757575757573,
      "test_accuracy": 0.32972972972972975,
      "test_f1": 0.28140397144664575,
      "test_precision": 0.2855079469713616,
      "test_recall": 0.32972972972972975,
      "learning_rate": 1e-06,
      "seed": 42,
      "checkpoint_dir": "./results_bert-base-arabertv02_fr_seed42"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "fr",
      "val_accuracy": 0.4666666666666667,
      "val_f1": 0.4265929104768749,
      "val_precision": 0.44089237578973656,
      "val_recall": 0.4666666666666667,
      "test_accuracy": 0.4702702702702703,
      "test_f1": 0.45045903045903046,
      "test_precision": 0.46507027608722523,
      "test_recall": 0.4702702702702703,
      "learning_rate": 1e-06,
      "seed": 43,
      "checkpoint_dir": "./results_bert-base-arabertv02_fr_seed43"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "fr",
      "val_accuracy": 0.4303030303030303,
      "val_f1": 0.3830270167222826,
      "val_precision": 0.3661140727461433,
      "val_recall": 0.4303030303030303,
      "test_accuracy": 0.43783783783783786,
      "test_f1": 0.3995965695800345,
      "test_precision": 0.38448344448344446,
      "test_recall": 0.43783783783783786,
      "learning_rate": 1e-06,
      "seed": 44,
      "checkpoint_dir": "./results_bert-base-arabertv02_fr_seed44"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "fr",
      "val_accuracy": 0.3939393939393939,
      "val_f1": 0.35262280658865097,
      "val_precision": 0.3503434082931181,
      "val_recall": 0.3939393939393939,
      "test_accuracy": 0.44324324324324327,
      "test_f1": 0.40856244175209694,
      "test_precision": 0.3874233696182184,
      "test_recall": 0.44324324324324327,
      "learning_rate": 1e-06,
      "seed": 45,
      "checkpoint_dir": "./results_bert-base-arabertv02_fr_seed45"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "fr",
      "val_accuracy": 0.3878787878787879,
      "val_f1": 0.3523822763334417,
      "val_precision": 0.40129966116662347,
      "val_recall": 0.3878787878787879,
      "test_accuracy": 0.4918918918918919,
      "test_f1": 0.4548101647617099,
      "test_precision": 0.4935989835989836,
      "test_recall": 0.4918918918918919,
      "learning_rate": 1e-06,
      "seed": 46,
      "checkpoint_dir": "./results_bert-base-arabertv02_fr_seed46"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "fr",
      "val_accuracy": 0.30303030303030304,
      "val_f1": 0.16023088023088025,
      "val_precision": 0.11998465669351745,
      "val_recall": 0.30303030303030304,
      "test_accuracy": 0.2756756756756757,
      "test_f1": 0.13782706192047373,
      "test_precision": 0.14765765765765765,
      "test_recall": 0.2756756756756757,
      "learning_rate": 1e-07,
      "seed": 42,
      "checkpoint_dir": "./results_bert-base-arabertv02_fr_seed42"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "fr",
      "val_accuracy": 0.3212121212121212,
      "val_f1": 0.16305144467935165,
      "val_precision": 0.14296337609221044,
      "val_recall": 0.3212121212121212,
      "test_accuracy": 0.2594594594594595,
      "test_f1": 0.10959925442684063,
      "test_precision": 0.06947275143996455,
      "test_recall": 0.2594594594594595,
      "learning_rate": 1e-07,
      "seed": 43,
      "checkpoint_dir": "./results_bert-base-arabertv02_fr_seed43"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "fr",
      "val_accuracy": 0.3090909090909091,
      "val_f1": 0.17370655234732904,
      "val_precision": 0.14946871310507673,
      "val_recall": 0.3090909090909091,
      "test_accuracy": 0.24324324324324326,
      "test_f1": 0.10501250148827242,
      "test_precision": 0.06696021864561191,
      "test_recall": 0.24324324324324326,
      "learning_rate": 1e-07,
      "seed": 44,
      "checkpoint_dir": "./results_bert-base-arabertv02_fr_seed44"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "fr",
      "val_accuracy": 0.23030303030303031,
      "val_f1": 0.17918073420465766,
      "val_precision": 0.26386427132189844,
      "val_recall": 0.23030303030303031,
      "test_accuracy": 0.24324324324324326,
      "test_f1": 0.19395482688839744,
      "test_precision": 0.4080951154220943,
      "test_recall": 0.24324324324324326,
      "learning_rate": 1e-07,
      "seed": 45,
      "checkpoint_dir": "./results_bert-base-arabertv02_fr_seed45"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "fr",
      "val_accuracy": 0.30303030303030304,
      "val_f1": 0.19468688661235242,
      "val_precision": 0.2997245179063361,
      "val_recall": 0.30303030303030304,
      "test_accuracy": 0.3027027027027027,
      "test_f1": 0.20283066804805935,
      "test_precision": 0.3586483015762047,
      "test_recall": 0.3027027027027027,
      "learning_rate": 1e-07,
      "seed": 46,
      "checkpoint_dir": "./results_bert-base-arabertv02_fr_seed46"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ru",
      "val_accuracy": 0.3977272727272727,
      "val_f1": 0.360666841386964,
      "val_precision": 0.3616893064043799,
      "val_recall": 0.3977272727272727,
      "test_accuracy": 0.33707865168539325,
      "test_f1": 0.30454914835904584,
      "test_precision": 0.3394459389021583,
      "test_recall": 0.33707865168539325,
      "learning_rate": 1e-05,
      "seed": 42,
      "checkpoint_dir": "./results_bert-base-arabertv02_ru_seed42"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ru",
      "val_accuracy": 0.4034090909090909,
      "val_f1": 0.3555939304446623,
      "val_precision": 0.3249245946963773,
      "val_recall": 0.4034090909090909,
      "test_accuracy": 0.34831460674157305,
      "test_f1": 0.3146082955974369,
      "test_precision": 0.38903001682679256,
      "test_recall": 0.34831460674157305,
      "learning_rate": 1e-05,
      "seed": 43,
      "checkpoint_dir": "./results_bert-base-arabertv02_ru_seed43"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ru",
      "val_accuracy": 0.4431818181818182,
      "val_f1": 0.3973711933249437,
      "val_precision": 0.4433412326220332,
      "val_recall": 0.4431818181818182,
      "test_accuracy": 0.33707865168539325,
      "test_f1": 0.29707594722929254,
      "test_precision": 0.3274863996548165,
      "test_recall": 0.33707865168539325,
      "learning_rate": 1e-05,
      "seed": 44,
      "checkpoint_dir": "./results_bert-base-arabertv02_ru_seed44"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ru",
      "val_accuracy": 0.4090909090909091,
      "val_f1": 0.35850095096534973,
      "val_precision": 0.4101727865160605,
      "val_recall": 0.4090909090909091,
      "test_accuracy": 0.3707865168539326,
      "test_f1": 0.3382400478079117,
      "test_precision": 0.40703605918257235,
      "test_recall": 0.3707865168539326,
      "learning_rate": 1e-05,
      "seed": 45,
      "checkpoint_dir": "./results_bert-base-arabertv02_ru_seed45"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ru",
      "val_accuracy": 0.4090909090909091,
      "val_f1": 0.3800201157538114,
      "val_precision": 0.4101413066838598,
      "val_recall": 0.4090909090909091,
      "test_accuracy": 0.3258426966292135,
      "test_f1": 0.2975558955735191,
      "test_precision": 0.3341490856866374,
      "test_recall": 0.3258426966292135,
      "learning_rate": 1e-05,
      "seed": 46,
      "checkpoint_dir": "./results_bert-base-arabertv02_ru_seed46"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ru",
      "val_accuracy": 0.3465909090909091,
      "val_f1": 0.26260504201680673,
      "val_precision": 0.22882846320346317,
      "val_recall": 0.3465909090909091,
      "test_accuracy": 0.3258426966292135,
      "test_f1": 0.26900757614606596,
      "test_precision": 0.4369077217308153,
      "test_recall": 0.3258426966292135,
      "learning_rate": 1e-06,
      "seed": 42,
      "checkpoint_dir": "./results_bert-base-arabertv02_ru_seed42"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ru",
      "val_accuracy": 0.32954545454545453,
      "val_f1": 0.28306816700170423,
      "val_precision": 0.287400936239869,
      "val_recall": 0.32954545454545453,
      "test_accuracy": 0.33707865168539325,
      "test_f1": 0.2928083245364626,
      "test_precision": 0.27712514672274274,
      "test_recall": 0.33707865168539325,
      "learning_rate": 1e-06,
      "seed": 43,
      "checkpoint_dir": "./results_bert-base-arabertv02_ru_seed43"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ru",
      "val_accuracy": 0.36363636363636365,
      "val_f1": 0.2974883604553878,
      "val_precision": 0.26987334542766916,
      "val_recall": 0.36363636363636365,
      "test_accuracy": 0.39325842696629215,
      "test_f1": 0.33505603963426955,
      "test_precision": 0.48476262871350406,
      "test_recall": 0.39325842696629215,
      "learning_rate": 1e-06,
      "seed": 44,
      "checkpoint_dir": "./results_bert-base-arabertv02_ru_seed44"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ru",
      "val_accuracy": 0.3977272727272727,
      "val_f1": 0.353769034717516,
      "val_precision": 0.3800208453133985,
      "val_recall": 0.3977272727272727,
      "test_accuracy": 0.34831460674157305,
      "test_f1": 0.30533727671819355,
      "test_precision": 0.3197082906133481,
      "test_recall": 0.34831460674157305,
      "learning_rate": 1e-06,
      "seed": 45,
      "checkpoint_dir": "./results_bert-base-arabertv02_ru_seed45"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ru",
      "val_accuracy": 0.3693181818181818,
      "val_f1": 0.28398889227853846,
      "val_precision": 0.23174531546483965,
      "val_recall": 0.3693181818181818,
      "test_accuracy": 0.34831460674157305,
      "test_f1": 0.27951904774274916,
      "test_precision": 0.2360933264825987,
      "test_recall": 0.34831460674157305,
      "learning_rate": 1e-06,
      "seed": 46,
      "checkpoint_dir": "./results_bert-base-arabertv02_ru_seed46"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ru",
      "val_accuracy": 0.29545454545454547,
      "val_f1": 0.19082491582491581,
      "val_precision": 0.15597098214285712,
      "val_recall": 0.29545454545454547,
      "test_accuracy": 0.2808988764044944,
      "test_f1": 0.19088219138485257,
      "test_precision": 0.35863403918179193,
      "test_recall": 0.2808988764044944,
      "learning_rate": 1e-07,
      "seed": 42,
      "checkpoint_dir": "./results_bert-base-arabertv02_ru_seed42"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ru",
      "val_accuracy": 0.26704545454545453,
      "val_f1": 0.20168542395104894,
      "val_precision": 0.17972337006427916,
      "val_recall": 0.26704545454545453,
      "test_accuracy": 0.25280898876404495,
      "test_f1": 0.19287065658354546,
      "test_precision": 0.18233134807207918,
      "test_recall": 0.25280898876404495,
      "learning_rate": 1e-07,
      "seed": 43,
      "checkpoint_dir": "./results_bert-base-arabertv02_ru_seed43"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ru",
      "val_accuracy": 0.25,
      "val_f1": 0.17333192661668081,
      "val_precision": 0.2804105386886844,
      "val_recall": 0.25,
      "test_accuracy": 0.23595505617977527,
      "test_f1": 0.15443790651801453,
      "test_precision": 0.29149277688603525,
      "test_recall": 0.23595505617977527,
      "learning_rate": 1e-07,
      "seed": 44,
      "checkpoint_dir": "./results_bert-base-arabertv02_ru_seed44"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ru",
      "val_accuracy": 0.36363636363636365,
      "val_f1": 0.337696556302547,
      "val_precision": 0.3384154889797845,
      "val_recall": 0.36363636363636365,
      "test_accuracy": 0.3539325842696629,
      "test_f1": 0.3321628766037768,
      "test_precision": 0.3269464986475675,
      "test_recall": 0.3539325842696629,
      "learning_rate": 1e-07,
      "seed": 45,
      "checkpoint_dir": "./results_bert-base-arabertv02_ru_seed45"
    },
    {
      "model_name": "aubmindlab/bert-base-arabertv02",
      "train_lang": "ru",
      "val_accuracy": 0.2784090909090909,
      "val_f1": 0.17957647191518158,
      "val_precision": 0.23676948051948052,
      "val_recall": 0.2784090909090909,
      "test_accuracy": 0.21348314606741572,
      "test_f1": 0.13944876596561992,
      "test_precision": 0.1625254935587129,
      "test_recall": 0.21348314606741572,
      "learning_rate": 1e-07,
      "seed": 46,
      "checkpoint_dir": "./results_bert-base-arabertv02_ru_seed46"
    }
  ]
}