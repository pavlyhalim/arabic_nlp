{
  "model": "hi",
  "best_config": {
    "model_name": "google/muril-base-cased",
    "train_lang": "ru",
    "learning_rate": 1e-05,
    "seed": 42,
    "val_accuracy": 0.375,
    "test_accuracy": 0.4101123595505618
  },
  "all_results": [
    {
      "model_name": "google/muril-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.375,
      "val_f1": 0.2864561884666494,
      "val_precision": 0.23425509149940968,
      "val_recall": 0.375,
      "test_accuracy": 0.4101123595505618,
      "test_f1": 0.3128578295465275,
      "test_precision": 0.26152765773552294,
      "test_recall": 0.4101123595505618,
      "learning_rate": 1e-05,
      "seed": 42,
      "checkpoint_dir": "./results_muril-base-cased_ru_seed42"
    },
    {
      "model_name": "google/muril-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.35795454545454547,
      "val_f1": 0.27507808390558003,
      "val_precision": 0.23659535697887968,
      "val_recall": 0.35795454545454547,
      "test_accuracy": 0.38764044943820225,
      "test_f1": 0.3067194116184949,
      "test_precision": 0.2622703763153201,
      "test_recall": 0.38764044943820225,
      "learning_rate": 1e-05,
      "seed": 43,
      "checkpoint_dir": "./results_muril-base-cased_ru_seed43"
    },
    {
      "model_name": "google/muril-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.3465909090909091,
      "val_f1": 0.24874443966135748,
      "val_precision": 0.22404796816087139,
      "val_recall": 0.3465909090909091,
      "test_accuracy": 0.3539325842696629,
      "test_f1": 0.25691672174487773,
      "test_precision": 0.22429349676540686,
      "test_recall": 0.3539325842696629,
      "learning_rate": 1e-05,
      "seed": 44,
      "checkpoint_dir": "./results_muril-base-cased_ru_seed44"
    },
    {
      "model_name": "google/muril-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.3181818181818182,
      "val_f1": 0.2097847149238045,
      "val_precision": 0.17708487312145849,
      "val_recall": 0.3181818181818182,
      "test_accuracy": 0.33707865168539325,
      "test_f1": 0.2233933813922747,
      "test_precision": 0.19107414181885046,
      "test_recall": 0.33707865168539325,
      "learning_rate": 1e-05,
      "seed": 45,
      "checkpoint_dir": "./results_muril-base-cased_ru_seed45"
    },
    {
      "model_name": "google/muril-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.32386363636363635,
      "val_f1": 0.2096504820936639,
      "val_precision": 0.16745887689363506,
      "val_recall": 0.32386363636363635,
      "test_accuracy": 0.3202247191011236,
      "test_f1": 0.20977446201454167,
      "test_precision": 0.1657266715865716,
      "test_recall": 0.3202247191011236,
      "learning_rate": 1e-05,
      "seed": 46,
      "checkpoint_dir": "./results_muril-base-cased_ru_seed46"
    },
    {
      "model_name": "google/muril-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.18181818181818182,
      "val_f1": 0.055944055944055944,
      "val_precision": 0.03305785123966942,
      "val_recall": 0.18181818181818182,
      "test_accuracy": 0.19101123595505617,
      "test_f1": 0.06126775492898028,
      "test_precision": 0.03648529226107814,
      "test_recall": 0.19101123595505617,
      "learning_rate": 1e-06,
      "seed": 42,
      "checkpoint_dir": "./results_muril-base-cased_ru_seed42"
    },
    {
      "model_name": "google/muril-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.2215909090909091,
      "val_f1": 0.08039112050739959,
      "val_precision": 0.04910253099173554,
      "val_recall": 0.2215909090909091,
      "test_accuracy": 0.23595505617977527,
      "test_f1": 0.09009193054136876,
      "test_precision": 0.055674788536800905,
      "test_recall": 0.23595505617977527,
      "learning_rate": 1e-06,
      "seed": 43,
      "checkpoint_dir": "./results_muril-base-cased_ru_seed43"
    },
    {
      "model_name": "google/muril-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.18181818181818182,
      "val_f1": 0.055944055944055944,
      "val_precision": 0.03305785123966942,
      "val_recall": 0.18181818181818182,
      "test_accuracy": 0.19101123595505617,
      "test_f1": 0.06126775492898028,
      "test_precision": 0.03648529226107814,
      "test_recall": 0.19101123595505617,
      "learning_rate": 1e-06,
      "seed": 44,
      "checkpoint_dir": "./results_muril-base-cased_ru_seed44"
    },
    {
      "model_name": "google/muril-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.2159090909090909,
      "val_f1": 0.07667799490229397,
      "val_precision": 0.046616735537190084,
      "val_recall": 0.2159090909090909,
      "test_accuracy": 0.20786516853932585,
      "test_f1": 0.07154429056702379,
      "test_precision": 0.043207928291882344,
      "test_recall": 0.20786516853932585,
      "learning_rate": 1e-06,
      "seed": 45,
      "checkpoint_dir": "./results_muril-base-cased_ru_seed45"
    },
    {
      "model_name": "google/muril-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.2159090909090909,
      "val_f1": 0.07667799490229397,
      "val_precision": 0.046616735537190084,
      "val_recall": 0.2159090909090909,
      "test_accuracy": 0.20786516853932585,
      "test_f1": 0.07154429056702379,
      "test_precision": 0.043207928291882344,
      "test_recall": 0.20786516853932585,
      "learning_rate": 1e-06,
      "seed": 46,
      "checkpoint_dir": "./results_muril-base-cased_ru_seed46"
    },
    {
      "model_name": "google/muril-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.18181818181818182,
      "val_f1": 0.055944055944055944,
      "val_precision": 0.03305785123966942,
      "val_recall": 0.18181818181818182,
      "test_accuracy": 0.19101123595505617,
      "test_f1": 0.06126775492898028,
      "test_precision": 0.03648529226107814,
      "test_recall": 0.19101123595505617,
      "learning_rate": 1e-07,
      "seed": 42,
      "checkpoint_dir": "./results_muril-base-cased_ru_seed42"
    },
    {
      "model_name": "google/muril-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.20454545454545456,
      "val_f1": 0.11949061301392906,
      "val_precision": 0.11053719008264462,
      "val_recall": 0.20454545454545456,
      "test_accuracy": 0.19101123595505617,
      "test_f1": 0.10353130016051365,
      "test_precision": 0.08656792645556691,
      "test_recall": 0.19101123595505617,
      "learning_rate": 1e-07,
      "seed": 43,
      "checkpoint_dir": "./results_muril-base-cased_ru_seed43"
    },
    {
      "model_name": "google/muril-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.18181818181818182,
      "val_f1": 0.07093481762881391,
      "val_precision": 0.05051261114135366,
      "val_recall": 0.18181818181818182,
      "test_accuracy": 0.1797752808988764,
      "test_f1": 0.06362591530219218,
      "test_precision": 0.04003132179450243,
      "test_recall": 0.1797752808988764,
      "learning_rate": 1e-07,
      "seed": 44,
      "checkpoint_dir": "./results_muril-base-cased_ru_seed44"
    },
    {
      "model_name": "google/muril-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.2159090909090909,
      "val_f1": 0.07667799490229397,
      "val_precision": 0.046616735537190084,
      "val_recall": 0.2159090909090909,
      "test_accuracy": 0.20786516853932585,
      "test_f1": 0.07154429056702379,
      "test_precision": 0.043207928291882344,
      "test_recall": 0.20786516853932585,
      "learning_rate": 1e-07,
      "seed": 45,
      "checkpoint_dir": "./results_muril-base-cased_ru_seed45"
    },
    {
      "model_name": "google/muril-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.2159090909090909,
      "val_f1": 0.07740137221269296,
      "val_precision": 0.047152560083594565,
      "val_recall": 0.2159090909090909,
      "test_accuracy": 0.20786516853932585,
      "test_f1": 0.07221606794323997,
      "test_precision": 0.04369892747701736,
      "test_recall": 0.20786516853932585,
      "learning_rate": 1e-07,
      "seed": 46,
      "checkpoint_dir": "./results_muril-base-cased_ru_seed46"
    }
  ]
}