{
  "model": "ru",
  "best_config": {
    "model_name": "DeepPavlov/rubert-base-cased",
    "train_lang": "en",
    "learning_rate": 1e-05,
    "seed": 46,
    "val_accuracy": 0.5477031802120141,
    "test_accuracy": 0.5202702702702703
  },
  "all_results": [
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "en",
      "val_accuracy": 0.5265017667844523,
      "val_f1": 0.49894741089183603,
      "val_precision": 0.4812432785374097,
      "val_recall": 0.5265017667844523,
      "test_accuracy": 0.5405405405405406,
      "test_f1": 0.5115594207145547,
      "test_precision": 0.4948293867102516,
      "test_recall": 0.5405405405405406,
      "learning_rate": 1e-05,
      "seed": 42,
      "checkpoint_dir": "./results_rubert-base-cased_en_seed42"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "en",
      "val_accuracy": 0.519434628975265,
      "val_f1": 0.45385395369555337,
      "val_precision": 0.5341919807509113,
      "val_recall": 0.519434628975265,
      "test_accuracy": 0.4864864864864865,
      "test_f1": 0.4253217996084427,
      "test_precision": 0.4484527539962323,
      "test_recall": 0.4864864864864865,
      "learning_rate": 1e-05,
      "seed": 43,
      "checkpoint_dir": "./results_rubert-base-cased_en_seed43"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "en",
      "val_accuracy": 0.5335689045936396,
      "val_f1": 0.4472971114855332,
      "val_precision": 0.4775183266910843,
      "val_recall": 0.5335689045936396,
      "test_accuracy": 0.47297297297297297,
      "test_f1": 0.3958902428278302,
      "test_precision": 0.4283340056474385,
      "test_recall": 0.47297297297297297,
      "learning_rate": 1e-05,
      "seed": 44,
      "checkpoint_dir": "./results_rubert-base-cased_en_seed44"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "en",
      "val_accuracy": 0.5406360424028268,
      "val_f1": 0.5171521764868025,
      "val_precision": 0.5089755113042347,
      "val_recall": 0.5406360424028268,
      "test_accuracy": 0.5472972972972973,
      "test_f1": 0.5298360760779085,
      "test_precision": 0.5427442809838895,
      "test_recall": 0.5472972972972973,
      "learning_rate": 1e-05,
      "seed": 45,
      "checkpoint_dir": "./results_rubert-base-cased_en_seed45"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "en",
      "val_accuracy": 0.5477031802120141,
      "val_f1": 0.5257275050377908,
      "val_precision": 0.5195051301316593,
      "val_recall": 0.5477031802120141,
      "test_accuracy": 0.5202702702702703,
      "test_f1": 0.5037622674597411,
      "test_precision": 0.5164309709719058,
      "test_recall": 0.5202702702702703,
      "learning_rate": 1e-05,
      "seed": 46,
      "checkpoint_dir": "./results_rubert-base-cased_en_seed46"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "en",
      "val_accuracy": 0.3674911660777385,
      "val_f1": 0.2591422857171827,
      "val_precision": 0.33477543519979897,
      "val_recall": 0.3674911660777385,
      "test_accuracy": 0.36486486486486486,
      "test_f1": 0.2611189368110976,
      "test_precision": 0.3382543895936753,
      "test_recall": 0.36486486486486486,
      "learning_rate": 1e-06,
      "seed": 42,
      "checkpoint_dir": "./results_rubert-base-cased_en_seed42"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "en",
      "val_accuracy": 0.44876325088339225,
      "val_f1": 0.3640111341207002,
      "val_precision": 0.3376000705454042,
      "val_recall": 0.44876325088339225,
      "test_accuracy": 0.3918918918918919,
      "test_f1": 0.2964462046361086,
      "test_precision": 0.27462334654271775,
      "test_recall": 0.3918918918918919,
      "learning_rate": 1e-06,
      "seed": 43,
      "checkpoint_dir": "./results_rubert-base-cased_en_seed43"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "en",
      "val_accuracy": 0.38515901060070673,
      "val_f1": 0.26441655304573963,
      "val_precision": 0.31519126398895,
      "val_recall": 0.38515901060070673,
      "test_accuracy": 0.3716216216216216,
      "test_f1": 0.244388607981283,
      "test_precision": 0.259844483982415,
      "test_recall": 0.3716216216216216,
      "learning_rate": 1e-06,
      "seed": 44,
      "checkpoint_dir": "./results_rubert-base-cased_en_seed44"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "en",
      "val_accuracy": 0.3286219081272085,
      "val_f1": 0.1717780868000639,
      "val_precision": 0.20278363881333095,
      "val_recall": 0.3286219081272085,
      "test_accuracy": 0.32094594594594594,
      "test_f1": 0.16802536231884058,
      "test_precision": 0.2626226397630507,
      "test_recall": 0.32094594594594594,
      "learning_rate": 1e-06,
      "seed": 45,
      "checkpoint_dir": "./results_rubert-base-cased_en_seed45"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "en",
      "val_accuracy": 0.33568904593639576,
      "val_f1": 0.20529081151872672,
      "val_precision": 0.28078708515208206,
      "val_recall": 0.33568904593639576,
      "test_accuracy": 0.30743243243243246,
      "test_f1": 0.17600498601977405,
      "test_precision": 0.1860282462914042,
      "test_recall": 0.30743243243243246,
      "learning_rate": 1e-06,
      "seed": 46,
      "checkpoint_dir": "./results_rubert-base-cased_en_seed46"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "en",
      "val_accuracy": 0.2862190812720848,
      "val_f1": 0.22525423535654795,
      "val_precision": 0.20873644449859877,
      "val_recall": 0.2862190812720848,
      "test_accuracy": 0.30405405405405406,
      "test_f1": 0.22387735990321514,
      "test_precision": 0.18400586633144772,
      "test_recall": 0.30405405405405406,
      "learning_rate": 1e-07,
      "seed": 42,
      "checkpoint_dir": "./results_rubert-base-cased_en_seed42"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "en",
      "val_accuracy": 0.18021201413427562,
      "val_f1": 0.18537015216230693,
      "val_precision": 0.2484686185428235,
      "val_recall": 0.18021201413427562,
      "test_accuracy": 0.18243243243243243,
      "test_f1": 0.184943501073038,
      "test_precision": 0.2506402605192928,
      "test_recall": 0.18243243243243243,
      "learning_rate": 1e-07,
      "seed": 43,
      "checkpoint_dir": "./results_rubert-base-cased_en_seed43"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "en",
      "val_accuracy": 0.16607773851590105,
      "val_f1": 0.16584504802124264,
      "val_precision": 0.42490490305735856,
      "val_recall": 0.16607773851590105,
      "test_accuracy": 0.13175675675675674,
      "test_f1": 0.12453904796909912,
      "test_precision": 0.46064407092168547,
      "test_recall": 0.13175675675675674,
      "learning_rate": 1e-07,
      "seed": 44,
      "checkpoint_dir": "./results_rubert-base-cased_en_seed44"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "en",
      "val_accuracy": 0.303886925795053,
      "val_f1": 0.15152717943753327,
      "val_precision": 0.10092594980784608,
      "val_recall": 0.303886925795053,
      "test_accuracy": 0.31756756756756754,
      "test_f1": 0.17259635182650418,
      "test_precision": 0.21376639797692426,
      "test_recall": 0.31756756756756754,
      "learning_rate": 1e-07,
      "seed": 45,
      "checkpoint_dir": "./results_rubert-base-cased_en_seed45"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "en",
      "val_accuracy": 0.21908127208480566,
      "val_f1": 0.08433390870559317,
      "val_precision": 0.36868168750079156,
      "val_recall": 0.21908127208480566,
      "test_accuracy": 0.26013513513513514,
      "test_f1": 0.1147183672302657,
      "test_precision": 0.3792200505003273,
      "test_recall": 0.26013513513513514,
      "learning_rate": 1e-07,
      "seed": 46,
      "checkpoint_dir": "./results_rubert-base-cased_en_seed46"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ar",
      "val_accuracy": 0.4642857142857143,
      "val_f1": 0.38809244459524717,
      "val_precision": 0.4422634914311335,
      "val_recall": 0.4642857142857143,
      "test_accuracy": 0.4368932038834951,
      "test_f1": 0.382297878839061,
      "test_precision": 0.410336100004264,
      "test_recall": 0.4368932038834951,
      "learning_rate": 1e-05,
      "seed": 42,
      "checkpoint_dir": "./results_rubert-base-cased_ar_seed42"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ar",
      "val_accuracy": 0.4846938775510204,
      "val_f1": 0.39930095505432617,
      "val_precision": 0.3482535880533802,
      "val_recall": 0.4846938775510204,
      "test_accuracy": 0.441747572815534,
      "test_f1": 0.38228687160464864,
      "test_precision": 0.34075073411121015,
      "test_recall": 0.441747572815534,
      "learning_rate": 1e-05,
      "seed": 43,
      "checkpoint_dir": "./results_rubert-base-cased_ar_seed43"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ar",
      "val_accuracy": 0.4846938775510204,
      "val_f1": 0.41424883206294566,
      "val_precision": 0.3974749199628753,
      "val_recall": 0.4846938775510204,
      "test_accuracy": 0.48058252427184467,
      "test_f1": 0.43032052066294785,
      "test_precision": 0.42048420734424474,
      "test_recall": 0.48058252427184467,
      "learning_rate": 1e-05,
      "seed": 44,
      "checkpoint_dir": "./results_rubert-base-cased_ar_seed44"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ar",
      "val_accuracy": 0.4846938775510204,
      "val_f1": 0.40235892891955316,
      "val_precision": 0.5517602040816326,
      "val_recall": 0.4846938775510204,
      "test_accuracy": 0.46601941747572817,
      "test_f1": 0.4014330702904265,
      "test_precision": 0.3648384575299635,
      "test_recall": 0.46601941747572817,
      "learning_rate": 1e-05,
      "seed": 45,
      "checkpoint_dir": "./results_rubert-base-cased_ar_seed45"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ar",
      "val_accuracy": 0.4387755102040816,
      "val_f1": 0.35901584662996466,
      "val_precision": 0.3657204994984054,
      "val_recall": 0.4387755102040816,
      "test_accuracy": 0.39805825242718446,
      "test_f1": 0.34139673273120824,
      "test_precision": 0.3453383120796559,
      "test_recall": 0.39805825242718446,
      "learning_rate": 1e-05,
      "seed": 46,
      "checkpoint_dir": "./results_rubert-base-cased_ar_seed46"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ar",
      "val_accuracy": 0.3979591836734694,
      "val_f1": 0.3225026187202791,
      "val_precision": 0.30365088061059725,
      "val_recall": 0.3979591836734694,
      "test_accuracy": 0.3737864077669903,
      "test_f1": 0.29892725224391986,
      "test_precision": 0.2640272349010213,
      "test_recall": 0.3737864077669903,
      "learning_rate": 1e-06,
      "seed": 42,
      "checkpoint_dir": "./results_rubert-base-cased_ar_seed42"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ar",
      "val_accuracy": 0.3520408163265306,
      "val_f1": 0.22663977528389045,
      "val_precision": 0.17768838304552592,
      "val_recall": 0.3520408163265306,
      "test_accuracy": 0.3737864077669903,
      "test_f1": 0.25783064618016077,
      "test_precision": 0.20822866453934413,
      "test_recall": 0.3737864077669903,
      "learning_rate": 1e-06,
      "seed": 43,
      "checkpoint_dir": "./results_rubert-base-cased_ar_seed43"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ar",
      "val_accuracy": 0.3469387755102041,
      "val_f1": 0.22398677780971543,
      "val_precision": 0.17234693877551022,
      "val_recall": 0.3469387755102041,
      "test_accuracy": 0.3737864077669903,
      "test_f1": 0.25953384982511196,
      "test_precision": 0.21000550292834402,
      "test_recall": 0.3737864077669903,
      "learning_rate": 1e-06,
      "seed": 44,
      "checkpoint_dir": "./results_rubert-base-cased_ar_seed44"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ar",
      "val_accuracy": 0.39285714285714285,
      "val_f1": 0.27087477180458297,
      "val_precision": 0.2090217039196631,
      "val_recall": 0.39285714285714285,
      "test_accuracy": 0.39805825242718446,
      "test_f1": 0.28931368513652905,
      "test_precision": 0.23094337329261436,
      "test_recall": 0.39805825242718446,
      "learning_rate": 1e-06,
      "seed": 45,
      "checkpoint_dir": "./results_rubert-base-cased_ar_seed45"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ar",
      "val_accuracy": 0.3520408163265306,
      "val_f1": 0.249787244488814,
      "val_precision": 0.19369423445053696,
      "val_recall": 0.3520408163265306,
      "test_accuracy": 0.35436893203883496,
      "test_f1": 0.26943789471506685,
      "test_precision": 0.2187999219121622,
      "test_recall": 0.35436893203883496,
      "learning_rate": 1e-06,
      "seed": 46,
      "checkpoint_dir": "./results_rubert-base-cased_ar_seed46"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ar",
      "val_accuracy": 0.1989795918367347,
      "val_f1": 0.08115646506923781,
      "val_precision": 0.12372309579569532,
      "val_recall": 0.1989795918367347,
      "test_accuracy": 0.1407766990291262,
      "test_f1": 0.047197365226582175,
      "test_precision": 0.03279652309602694,
      "test_recall": 0.1407766990291262,
      "learning_rate": 1e-07,
      "seed": 42,
      "checkpoint_dir": "./results_rubert-base-cased_ar_seed42"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ar",
      "val_accuracy": 0.27040816326530615,
      "val_f1": 0.1734025766457479,
      "val_precision": 0.17579715283796915,
      "val_recall": 0.27040816326530615,
      "test_accuracy": 0.2961165048543689,
      "test_f1": 0.18068821100586202,
      "test_precision": 0.1454929355118794,
      "test_recall": 0.2961165048543689,
      "learning_rate": 1e-07,
      "seed": 43,
      "checkpoint_dir": "./results_rubert-base-cased_ar_seed43"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ar",
      "val_accuracy": 0.11224489795918367,
      "val_f1": 0.0636209404811267,
      "val_precision": 0.11681159174633306,
      "val_recall": 0.11224489795918367,
      "test_accuracy": 0.07281553398058252,
      "test_f1": 0.029778369418435723,
      "test_precision": 0.040666516680223135,
      "test_recall": 0.07281553398058252,
      "learning_rate": 1e-07,
      "seed": 44,
      "checkpoint_dir": "./results_rubert-base-cased_ar_seed44"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ar",
      "val_accuracy": 0.32142857142857145,
      "val_f1": 0.1569767441860465,
      "val_precision": 0.10384615384615385,
      "val_recall": 0.32142857142857145,
      "test_accuracy": 0.3300970873786408,
      "test_f1": 0.16444397026921298,
      "test_precision": 0.10949561922803695,
      "test_recall": 0.3300970873786408,
      "learning_rate": 1e-07,
      "seed": 45,
      "checkpoint_dir": "./results_rubert-base-cased_ar_seed45"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ar",
      "val_accuracy": 0.25510204081632654,
      "val_f1": 0.14614919963757175,
      "val_precision": 0.15799498746867166,
      "val_recall": 0.25510204081632654,
      "test_accuracy": 0.24271844660194175,
      "test_f1": 0.12156451757266873,
      "test_precision": 0.11232992765041765,
      "test_recall": 0.24271844660194175,
      "learning_rate": 1e-07,
      "seed": 46,
      "checkpoint_dir": "./results_rubert-base-cased_ar_seed46"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "hi",
      "val_accuracy": 0.4161073825503356,
      "val_f1": 0.36621569286683997,
      "val_precision": 0.35095299222816,
      "val_recall": 0.4161073825503356,
      "test_accuracy": 0.3619631901840491,
      "test_f1": 0.3175341029173245,
      "test_precision": 0.2917699080434753,
      "test_recall": 0.3619631901840491,
      "learning_rate": 1e-05,
      "seed": 42,
      "checkpoint_dir": "./results_rubert-base-cased_hi_seed42"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "hi",
      "val_accuracy": 0.33557046979865773,
      "val_f1": 0.24417066391530295,
      "val_precision": 0.2606516855719188,
      "val_recall": 0.33557046979865773,
      "test_accuracy": 0.3312883435582822,
      "test_f1": 0.20530293203068134,
      "test_precision": 0.15185735497385475,
      "test_recall": 0.3312883435582822,
      "learning_rate": 1e-05,
      "seed": 43,
      "checkpoint_dir": "./results_rubert-base-cased_hi_seed43"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "hi",
      "val_accuracy": 0.3691275167785235,
      "val_f1": 0.28963174993621876,
      "val_precision": 0.2583234636136334,
      "val_recall": 0.3691275167785235,
      "test_accuracy": 0.34355828220858897,
      "test_f1": 0.26730764567521037,
      "test_precision": 0.2679006453165078,
      "test_recall": 0.34355828220858897,
      "learning_rate": 1e-05,
      "seed": 44,
      "checkpoint_dir": "./results_rubert-base-cased_hi_seed44"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "hi",
      "val_accuracy": 0.3288590604026846,
      "val_f1": 0.26886434359727507,
      "val_precision": 0.2872499034005853,
      "val_recall": 0.3288590604026846,
      "test_accuracy": 0.3803680981595092,
      "test_f1": 0.30713489514201964,
      "test_precision": 0.311296111482172,
      "test_recall": 0.3803680981595092,
      "learning_rate": 1e-05,
      "seed": 45,
      "checkpoint_dir": "./results_rubert-base-cased_hi_seed45"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "hi",
      "val_accuracy": 0.42953020134228187,
      "val_f1": 0.29930639356658545,
      "val_precision": 0.24007670182166826,
      "val_recall": 0.42953020134228187,
      "test_accuracy": 0.31901840490797545,
      "test_f1": 0.22203078603476917,
      "test_precision": 0.36795695548320906,
      "test_recall": 0.31901840490797545,
      "learning_rate": 1e-05,
      "seed": 46,
      "checkpoint_dir": "./results_rubert-base-cased_hi_seed46"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "hi",
      "val_accuracy": 0.2684563758389262,
      "val_f1": 0.16256524981357195,
      "val_precision": 0.1792790424133398,
      "val_recall": 0.2684563758389262,
      "test_accuracy": 0.20245398773006135,
      "test_f1": 0.10837557185726257,
      "test_precision": 0.07965024108365697,
      "test_recall": 0.20245398773006135,
      "learning_rate": 1e-06,
      "seed": 42,
      "checkpoint_dir": "./results_rubert-base-cased_hi_seed42"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "hi",
      "val_accuracy": 0.21476510067114093,
      "val_f1": 0.13128511061396966,
      "val_precision": 0.11154956328204339,
      "val_recall": 0.21476510067114093,
      "test_accuracy": 0.22699386503067484,
      "test_f1": 0.15064508797935663,
      "test_precision": 0.1386116574201818,
      "test_recall": 0.22699386503067484,
      "learning_rate": 1e-06,
      "seed": 43,
      "checkpoint_dir": "./results_rubert-base-cased_hi_seed43"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "hi",
      "val_accuracy": 0.24161073825503357,
      "val_f1": 0.1561502990457928,
      "val_precision": 0.14968193755471257,
      "val_recall": 0.24161073825503357,
      "test_accuracy": 0.2392638036809816,
      "test_f1": 0.15589402644617367,
      "test_precision": 0.21520775369782305,
      "test_recall": 0.2392638036809816,
      "learning_rate": 1e-06,
      "seed": 44,
      "checkpoint_dir": "./results_rubert-base-cased_hi_seed44"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "hi",
      "val_accuracy": 0.24161073825503357,
      "val_f1": 0.13093508470362694,
      "val_precision": 0.4419324286810692,
      "val_recall": 0.24161073825503357,
      "test_accuracy": 0.2147239263803681,
      "test_f1": 0.1087105052232595,
      "test_precision": 0.19307040607654102,
      "test_recall": 0.2147239263803681,
      "learning_rate": 1e-06,
      "seed": 45,
      "checkpoint_dir": "./results_rubert-base-cased_hi_seed45"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "hi",
      "val_accuracy": 0.2483221476510067,
      "val_f1": 0.17415052391280766,
      "val_precision": 0.2771342109162442,
      "val_recall": 0.2483221476510067,
      "test_accuracy": 0.1901840490797546,
      "test_f1": 0.09864650355447901,
      "test_precision": 0.2866060975038636,
      "test_recall": 0.1901840490797546,
      "learning_rate": 1e-06,
      "seed": 46,
      "checkpoint_dir": "./results_rubert-base-cased_hi_seed46"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "hi",
      "val_accuracy": 0.1342281879194631,
      "val_f1": 0.0453777873998701,
      "val_precision": 0.02736189984512132,
      "val_recall": 0.1342281879194631,
      "test_accuracy": 0.15950920245398773,
      "test_f1": 0.06751122088545401,
      "test_precision": 0.042829031137863176,
      "test_recall": 0.15950920245398773,
      "learning_rate": 1e-07,
      "seed": 42,
      "checkpoint_dir": "./results_rubert-base-cased_hi_seed42"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "hi",
      "val_accuracy": 0.15436241610738255,
      "val_f1": 0.08652897828397461,
      "val_precision": 0.07427020243356797,
      "val_recall": 0.15436241610738255,
      "test_accuracy": 0.1656441717791411,
      "test_f1": 0.08565064750756263,
      "test_precision": 0.07622682372228469,
      "test_recall": 0.1656441717791411,
      "learning_rate": 1e-07,
      "seed": 43,
      "checkpoint_dir": "./results_rubert-base-cased_hi_seed43"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "hi",
      "val_accuracy": 0.2080536912751678,
      "val_f1": 0.15166006030256327,
      "val_precision": 0.14895496624301033,
      "val_recall": 0.2080536912751678,
      "test_accuracy": 0.1901840490797546,
      "test_f1": 0.13127542202072967,
      "test_precision": 0.13212117950574523,
      "test_recall": 0.1901840490797546,
      "learning_rate": 1e-07,
      "seed": 44,
      "checkpoint_dir": "./results_rubert-base-cased_hi_seed44"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "hi",
      "val_accuracy": 0.2080536912751678,
      "val_f1": 0.07166293810589112,
      "val_precision": 0.043286338453222835,
      "val_recall": 0.2080536912751678,
      "test_accuracy": 0.19631901840490798,
      "test_f1": 0.06443290860468774,
      "test_precision": 0.038541156987466595,
      "test_recall": 0.19631901840490798,
      "learning_rate": 1e-07,
      "seed": 45,
      "checkpoint_dir": "./results_rubert-base-cased_hi_seed45"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "hi",
      "val_accuracy": 0.2550335570469799,
      "val_f1": 0.11543257417390985,
      "val_precision": 0.1389030317056237,
      "val_recall": 0.2550335570469799,
      "test_accuracy": 0.15337423312883436,
      "test_f1": 0.04079101944915807,
      "test_precision": 0.02352365538785803,
      "test_recall": 0.15337423312883436,
      "learning_rate": 1e-07,
      "seed": 46,
      "checkpoint_dir": "./results_rubert-base-cased_hi_seed46"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "fr",
      "val_accuracy": 0.45454545454545453,
      "val_f1": 0.4060525757175242,
      "val_precision": 0.38267909674910366,
      "val_recall": 0.45454545454545453,
      "test_accuracy": 0.4594594594594595,
      "test_f1": 0.4186903629462576,
      "test_precision": 0.4133403699670677,
      "test_recall": 0.4594594594594595,
      "learning_rate": 1e-05,
      "seed": 42,
      "checkpoint_dir": "./results_rubert-base-cased_fr_seed42"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "fr",
      "val_accuracy": 0.42424242424242425,
      "val_f1": 0.35775509893156954,
      "val_precision": 0.42598173515981735,
      "val_recall": 0.42424242424242425,
      "test_accuracy": 0.4540540540540541,
      "test_f1": 0.38124441942681636,
      "test_precision": 0.5588974688974689,
      "test_recall": 0.4540540540540541,
      "learning_rate": 1e-05,
      "seed": 43,
      "checkpoint_dir": "./results_rubert-base-cased_fr_seed43"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "fr",
      "val_accuracy": 0.48484848484848486,
      "val_f1": 0.42851137710378834,
      "val_precision": 0.42035674520778793,
      "val_recall": 0.48484848484848486,
      "test_accuracy": 0.4648648648648649,
      "test_f1": 0.425288291454439,
      "test_precision": 0.41076203922503735,
      "test_recall": 0.4648648648648649,
      "learning_rate": 1e-05,
      "seed": 44,
      "checkpoint_dir": "./results_rubert-base-cased_fr_seed44"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "fr",
      "val_accuracy": 0.4484848484848485,
      "val_f1": 0.38745182969800096,
      "val_precision": 0.40046323338189366,
      "val_recall": 0.4484848484848485,
      "test_accuracy": 0.4972972972972973,
      "test_f1": 0.4415259104052964,
      "test_precision": 0.4386792149292149,
      "test_recall": 0.4972972972972973,
      "learning_rate": 1e-05,
      "seed": 45,
      "checkpoint_dir": "./results_rubert-base-cased_fr_seed45"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "fr",
      "val_accuracy": 0.4484848484848485,
      "val_f1": 0.42189331889257425,
      "val_precision": 0.4182599721993493,
      "val_recall": 0.4484848484848485,
      "test_accuracy": 0.4540540540540541,
      "test_f1": 0.42614525109151336,
      "test_precision": 0.42874943909426666,
      "test_recall": 0.4540540540540541,
      "learning_rate": 1e-05,
      "seed": 46,
      "checkpoint_dir": "./results_rubert-base-cased_fr_seed46"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "fr",
      "val_accuracy": 0.3575757575757576,
      "val_f1": 0.2445763841892874,
      "val_precision": 0.28716898716898714,
      "val_recall": 0.3575757575757576,
      "test_accuracy": 0.2648648648648649,
      "test_f1": 0.13764206920810293,
      "test_precision": 0.13677807985500295,
      "test_recall": 0.2648648648648649,
      "learning_rate": 1e-06,
      "seed": 42,
      "checkpoint_dir": "./results_rubert-base-cased_fr_seed42"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "fr",
      "val_accuracy": 0.296969696969697,
      "val_f1": 0.24852799101048317,
      "val_precision": 0.2769614261962017,
      "val_recall": 0.296969696969697,
      "test_accuracy": 0.2864864864864865,
      "test_f1": 0.24862718402541414,
      "test_precision": 0.22766093366093362,
      "test_recall": 0.2864864864864865,
      "learning_rate": 1e-06,
      "seed": 43,
      "checkpoint_dir": "./results_rubert-base-cased_fr_seed43"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "fr",
      "val_accuracy": 0.24848484848484848,
      "val_f1": 0.16162706230817686,
      "val_precision": 0.30852517695593484,
      "val_recall": 0.24848484848484848,
      "test_accuracy": 0.21081081081081082,
      "test_f1": 0.12435600435600436,
      "test_precision": 0.09462735462735464,
      "test_recall": 0.21081081081081082,
      "learning_rate": 1e-06,
      "seed": 44,
      "checkpoint_dir": "./results_rubert-base-cased_fr_seed44"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "fr",
      "val_accuracy": 0.2727272727272727,
      "val_f1": 0.20157943067033973,
      "val_precision": 0.2194087755467066,
      "val_recall": 0.2727272727272727,
      "test_accuracy": 0.3675675675675676,
      "test_f1": 0.2790567254370935,
      "test_precision": 0.25464451318109854,
      "test_recall": 0.3675675675675676,
      "learning_rate": 1e-06,
      "seed": 45,
      "checkpoint_dir": "./results_rubert-base-cased_fr_seed45"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "fr",
      "val_accuracy": 0.3151515151515151,
      "val_f1": 0.15173961840628505,
      "val_precision": 0.09992609016999261,
      "val_recall": 0.3151515151515151,
      "test_accuracy": 0.2648648648648649,
      "test_f1": 0.1118825722273998,
      "test_precision": 0.07092010042829716,
      "test_recall": 0.2648648648648649,
      "learning_rate": 1e-06,
      "seed": 46,
      "checkpoint_dir": "./results_rubert-base-cased_fr_seed46"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "fr",
      "val_accuracy": 0.19393939393939394,
      "val_f1": 0.17654193562152085,
      "val_precision": 0.2786150632662261,
      "val_recall": 0.19393939393939394,
      "test_accuracy": 0.13513513513513514,
      "test_f1": 0.1205406122673675,
      "test_precision": 0.13655558619844332,
      "test_recall": 0.13513513513513514,
      "learning_rate": 1e-07,
      "seed": 42,
      "checkpoint_dir": "./results_rubert-base-cased_fr_seed42"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "fr",
      "val_accuracy": 0.12121212121212122,
      "val_f1": 0.07427609427609429,
      "val_precision": 0.10929416412188088,
      "val_recall": 0.12121212121212122,
      "test_accuracy": 0.04864864864864865,
      "test_f1": 0.027810278973069674,
      "test_precision": 0.06265054528212421,
      "test_recall": 0.04864864864864865,
      "learning_rate": 1e-07,
      "seed": 43,
      "checkpoint_dir": "./results_rubert-base-cased_fr_seed43"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "fr",
      "val_accuracy": 0.17575757575757575,
      "val_f1": 0.11985455389844105,
      "val_precision": 0.12726791726791725,
      "val_recall": 0.17575757575757575,
      "test_accuracy": 0.12972972972972974,
      "test_f1": 0.08930145464072049,
      "test_precision": 0.15178179059008726,
      "test_recall": 0.12972972972972974,
      "learning_rate": 1e-07,
      "seed": 44,
      "checkpoint_dir": "./results_rubert-base-cased_fr_seed44"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "fr",
      "val_accuracy": 0.17575757575757575,
      "val_f1": 0.05281833882870153,
      "val_precision": 0.031079083518107908,
      "val_recall": 0.17575757575757575,
      "test_accuracy": 0.24864864864864866,
      "test_f1": 0.09902889902889903,
      "test_precision": 0.06182615047479913,
      "test_recall": 0.24864864864864866,
      "learning_rate": 1e-07,
      "seed": 45,
      "checkpoint_dir": "./results_rubert-base-cased_fr_seed45"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "fr",
      "val_accuracy": 0.3151515151515151,
      "val_f1": 0.1510403574919704,
      "val_precision": 0.09932047750229568,
      "val_recall": 0.3151515151515151,
      "test_accuracy": 0.2648648648648649,
      "test_f1": 0.11140238951397749,
      "test_precision": 0.0705346650998825,
      "test_recall": 0.2648648648648649,
      "learning_rate": 1e-07,
      "seed": 46,
      "checkpoint_dir": "./results_rubert-base-cased_fr_seed46"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.48295454545454547,
      "val_f1": 0.4105372374098323,
      "val_precision": 0.3626299203892083,
      "val_recall": 0.48295454545454547,
      "test_accuracy": 0.48314606741573035,
      "test_f1": 0.4088527900589686,
      "test_precision": 0.36031240415981264,
      "test_recall": 0.48314606741573035,
      "learning_rate": 1e-05,
      "seed": 42,
      "checkpoint_dir": "./results_rubert-base-cased_ru_seed42"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.4602272727272727,
      "val_f1": 0.367089072543618,
      "val_precision": 0.34713419660011424,
      "val_recall": 0.4602272727272727,
      "test_accuracy": 0.5056179775280899,
      "test_f1": 0.42318974449094054,
      "test_precision": 0.3851140878912205,
      "test_recall": 0.5056179775280899,
      "learning_rate": 1e-05,
      "seed": 43,
      "checkpoint_dir": "./results_rubert-base-cased_ru_seed43"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.4772727272727273,
      "val_f1": 0.39904259797552477,
      "val_precision": 0.4209535573122529,
      "val_recall": 0.4772727272727273,
      "test_accuracy": 0.5168539325842697,
      "test_f1": 0.4317429954779819,
      "test_precision": 0.4315884036463854,
      "test_recall": 0.5168539325842697,
      "learning_rate": 1e-05,
      "seed": 44,
      "checkpoint_dir": "./results_rubert-base-cased_ru_seed44"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.4602272727272727,
      "val_f1": 0.39305708965217023,
      "val_precision": 0.4391758241758242,
      "val_recall": 0.4602272727272727,
      "test_accuracy": 0.4887640449438202,
      "test_f1": 0.41948243058500895,
      "test_precision": 0.5629707621142866,
      "test_recall": 0.4887640449438202,
      "learning_rate": 1e-05,
      "seed": 45,
      "checkpoint_dir": "./results_rubert-base-cased_ru_seed45"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.4318181818181818,
      "val_f1": 0.3584576590627204,
      "val_precision": 0.3098408624933425,
      "val_recall": 0.4318181818181818,
      "test_accuracy": 0.4943820224719101,
      "test_f1": 0.43519595429316277,
      "test_precision": 0.4790084718760203,
      "test_recall": 0.4943820224719101,
      "learning_rate": 1e-05,
      "seed": 46,
      "checkpoint_dir": "./results_rubert-base-cased_ru_seed46"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.26704545454545453,
      "val_f1": 0.16770424366643363,
      "val_precision": 0.2217235721267979,
      "val_recall": 0.26704545454545453,
      "test_accuracy": 0.24719101123595505,
      "test_f1": 0.16375624219725343,
      "test_precision": 0.30402319681043855,
      "test_recall": 0.24719101123595505,
      "learning_rate": 1e-06,
      "seed": 42,
      "checkpoint_dir": "./results_rubert-base-cased_ru_seed42"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.23863636363636365,
      "val_f1": 0.1539586101025368,
      "val_precision": 0.3568461140178766,
      "val_recall": 0.23863636363636365,
      "test_accuracy": 0.21910112359550563,
      "test_f1": 0.1422436282569954,
      "test_precision": 0.24824521005116426,
      "test_recall": 0.21910112359550563,
      "learning_rate": 1e-06,
      "seed": 43,
      "checkpoint_dir": "./results_rubert-base-cased_ru_seed43"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.3125,
      "val_f1": 0.20587479912764006,
      "val_precision": 0.19676226551226553,
      "val_recall": 0.3125,
      "test_accuracy": 0.38764044943820225,
      "test_f1": 0.2649303047634283,
      "test_precision": 0.22261921761677503,
      "test_recall": 0.38764044943820225,
      "learning_rate": 1e-06,
      "seed": 44,
      "checkpoint_dir": "./results_rubert-base-cased_ru_seed44"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.20454545454545456,
      "val_f1": 0.1366309393089578,
      "val_precision": 0.19920578159214522,
      "val_recall": 0.20454545454545456,
      "test_accuracy": 0.2247191011235955,
      "test_f1": 0.1588583467094703,
      "test_precision": 0.24564811085288432,
      "test_recall": 0.2247191011235955,
      "learning_rate": 1e-06,
      "seed": 45,
      "checkpoint_dir": "./results_rubert-base-cased_ru_seed45"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.2215909090909091,
      "val_f1": 0.08811753094323517,
      "val_precision": 0.268474025974026,
      "val_recall": 0.2215909090909091,
      "test_accuracy": 0.21348314606741572,
      "test_f1": 0.08294129776959339,
      "test_precision": 0.161676455566905,
      "test_recall": 0.21348314606741572,
      "learning_rate": 1e-06,
      "seed": 46,
      "checkpoint_dir": "./results_rubert-base-cased_ru_seed46"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.19886363636363635,
      "val_f1": 0.14832298191196924,
      "val_precision": 0.33027597402597403,
      "val_recall": 0.19886363636363635,
      "test_accuracy": 0.19662921348314608,
      "test_f1": 0.14880432138255406,
      "test_precision": 0.3456491074468602,
      "test_recall": 0.19662921348314608,
      "learning_rate": 1e-07,
      "seed": 42,
      "checkpoint_dir": "./results_rubert-base-cased_ru_seed42"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.10227272727272728,
      "val_f1": 0.08710884076847933,
      "val_precision": 0.0806686999868818,
      "val_recall": 0.10227272727272728,
      "test_accuracy": 0.10112359550561797,
      "test_f1": 0.08583076340496529,
      "test_precision": 0.07639612264472793,
      "test_recall": 0.10112359550561797,
      "learning_rate": 1e-07,
      "seed": 43,
      "checkpoint_dir": "./results_rubert-base-cased_ru_seed43"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.13636363636363635,
      "val_f1": 0.07747040469364007,
      "val_precision": 0.08715417985375967,
      "val_recall": 0.13636363636363635,
      "test_accuracy": 0.15730337078651685,
      "test_f1": 0.10015805540861786,
      "test_precision": 0.1350288135147225,
      "test_recall": 0.15730337078651685,
      "learning_rate": 1e-07,
      "seed": 44,
      "checkpoint_dir": "./results_rubert-base-cased_ru_seed44"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.1534090909090909,
      "val_f1": 0.08519640852974186,
      "val_precision": 0.06881969381969383,
      "val_recall": 0.1534090909090909,
      "test_accuracy": 0.15730337078651685,
      "test_f1": 0.1000333998086807,
      "test_precision": 0.08189736568784818,
      "test_recall": 0.15730337078651685,
      "learning_rate": 1e-07,
      "seed": 45,
      "checkpoint_dir": "./results_rubert-base-cased_ru_seed45"
    },
    {
      "model_name": "DeepPavlov/rubert-base-cased",
      "train_lang": "ru",
      "val_accuracy": 0.20454545454545456,
      "val_f1": 0.08928571428571429,
      "val_precision": 0.05723624201885072,
      "val_recall": 0.20454545454545456,
      "test_accuracy": 0.16292134831460675,
      "test_f1": 0.07899754148240237,
      "test_precision": 0.27957002565690525,
      "test_recall": 0.16292134831460675,
      "learning_rate": 1e-07,
      "seed": 46,
      "checkpoint_dir": "./results_rubert-base-cased_ru_seed46"
    }
  ]
}